### 磁盘优化技术

- 提高系统吞吐量
  - 零拷贝
  - 直接I/O
  - 异步I/O
- 减少磁盘访问次数
  - 磁盘高速缓存区
    - 页高速缓存(Page Cache)是Linux内核所使用的主要磁盘高速缓存

### 常规方法 vs DMA

- DMA: 直接内存访问(Direct Memory Access)

什么是 DMA 技术？简单理解就是，在进行 I/O 设备和内存的数据传输的时候，数据搬运的工作全部交给 DMA 控制器，而 CPU 不再参与任何与数据搬运相关的事情，这样 CPU 就可以去处理别的事务。

**传统的从磁盘读取数据的流程**

![](/static/images/2108/p004.png)

可以看到，整个数据的传输过程，都要需要 CPU 亲自参与搬运数据的过程，而且这个过程，CPU 是不能做其他事情的。

简单的搬运几个字符数据那没问题，但是如果我们用千兆网卡或者硬盘传输大量数据的时候，都用 CPU 来搬运的话，肯定忙不过来。

**基于DMA的方式从磁盘读取数据的流程**

![](/static/images/2108/p005.png)

早期 DMA 只存在在主板上，如今由于 I/O 设备越来越多，数据传输的需求也不尽相同，所以每个 I/O 设备里面都有自己的 DMA 控制器。

### 将磁盘的文件通过网络发送给用户

常规流程：将磁盘上的文件读取出来，然后通过网络协议发送给客户端。对应的系统调用如下：

```c
read(file, tmp_buf, len);
write(socket, tmp_buf, len);
```

虽然代码简单，但流程较为复杂，如下：

![](/static/images/2108/p006.png)

- 发生了 4 次用户态与内核态的上下文切换
  - 因为发生了两次系统调用，一次是 read() ，一次是 write()，每次系统调用都得先从用户态切换到内核态，等内核完成任务后，再从内核态切换回用户态。
  - 上下文切换到成本并不小，一次切换需要耗时几十纳秒到几微秒，虽然时间看上去很短，但是在高并发的场景下，这类时间容易被累积和放大，从而影响系统的性能
- 其次，还发生了 4 次数据拷贝
  - 其中两次是 DMA 的拷贝，另外两次则是通过 CPU 拷贝的

要想提高文件传输的性能，就需要减少「用户态与内核态的上下文切换」和「内存拷贝」的次数

### 优化思路

用户的缓冲区是没有必要存在的

### 如何优化(如何实现零拷贝)

零拷贝技术实现的方式通常有 2 种：

- mmap + write
- sendfile

#### mmap+write

```c
buf = mmap(file, len);
write(sockfd, buf, len);
```

- mmap() 系统调用函数会直接把内核缓冲区里的数据「映射」到用户空间，这样，操作系统内核与用户空间就不需要再进行任何的数据拷贝操作

![](/static/images/2108/p007.png)

- 仍然需要 4 次上下文切换，因为系统调用还是 2 次
- 通过使用 mmap() 来代替 read()，数据拷贝优化到3次

#### sendfile，普通DMA

kernel>=2.1

```c
#include <sys/socket.h>
ssize_t sendfile(int out_fd, int in_fd, off_t *offset, size_t count);
```

![](/static/images/2108/p008.png)

- 替代前面的 read() 和 write() 这两个系统调用，这样就可以减少一次系统调用，也就减少了 2 次上下文切换的开销。
- 3 次数据拷贝

#### sendfile，SG-DMA

kernel>=2.4

如果网卡支持 SG-DMA（The Scatter-Gather Direct Memory Access）技术（和普通的 DMA 有所不同），我们可以进一步减少通过 CPU 把内核缓冲区里的数据拷贝到 socket 缓冲区的过程。

查看网卡是否支持 scatter-gather 特性

```bash
ethtool -k eth0 | grep scatter-gather
scatter-gather: on
```

![](/static/images/2108/p009.png)

- 2次上下文切换
- 2次数据拷贝次数
  - 而且 2 次的数据拷贝过程，都不需要通过 CPU，2 次都是由 DMA 来搬运

这就是所谓的零拷贝（Zero-copy）技术，因为我们没有在内存层面去拷贝数据，也就是说全程没有通过 CPU 来搬运数据，所有的数据都是通过 DMA 来进行传输的。

总体来看，零拷贝技术可以把文件传输的性能提高至少一倍以上。

### 使用零拷贝技术的项目

- kafka
- Nginx 也支持零拷贝技术，一般默认是开启零拷贝技术，这样有利于提高文件传输的效率
  - `sendfile on`

### PageCache的作用与问题

由于零拷贝使用了 PageCache 技术，可以使得零拷贝进一步提升了性能。

PageCache 的优点主要是两个。这两个做法，将大大提高读写磁盘的性能。

- 缓存最近被访问的数据；
- 预读功能；

但是，在传输大文件（GB 级别的文件）的时候，PageCache 会不起作用，那就白白浪费 DMA 多做的一次数据拷贝，造成性能的降低，即使使用了 PageCache 的零拷贝也会损失性能

所以，针对大文件的传输，不应该使用 PageCache，也就是说不应该使用零拷贝技术，因为可能由于 PageCache 被大文件占据，而导致「热点」小文件无法利用到 PageCache，这样在高并发的环境下，会带来严重的性能问题。

### 大文件传输用什么方式实现？

常规流程

![](/static/images/2108/p010.png)

优化流程

![](/static/images/2108/p011.png)

优化点：

- 使用异步 I/O，解决阻塞问题
- 使用直接I/O，绕开PageCache

知识点：

- 绕开 PageCache 的 I/O 叫直接 I/O
- 使用 PageCache 的 I/O 则叫缓存 I/O
- 通常，对于磁盘，异步 I/O 只支持直接 I/O。

在高并发的场景下，针对大文件的传输的方式，应该使用「异步 I/O + 直接 I/O」来替代零拷贝技术

总结：传输文件的时候，我们要根据文件的大小来使用不同的方式

- 传输大文件的时候，使用「异步 I/O + 直接 I/O」
- 传输小文件的时候，则使用「零拷贝技术」

例如nginx配置如下，当文件大小大于 directio 值后，使用「异步 I/O + 直接 I/O」，否则使用「零拷贝技术」：

```
location /video/ { 
    sendfile on; 
    aio on; 
    directio 1024m; 
}
```

## 参考

- [原来 8 张图，就可以搞懂「零拷贝」了](https://www.cnblogs.com/xiaolincoding/p/13719610.html)
- [磁盘高速缓存](https://blog.csdn.net/yunsongice/article/details/5833154)